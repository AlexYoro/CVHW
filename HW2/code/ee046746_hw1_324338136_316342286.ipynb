{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/python.png\" style=\"height:50px;display:inline\"> Import Python Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from PIL import Image\n",
    "from skimage.io import imread\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# define device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('PIL',torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1 - Classic Vs. Deep Learning-based Semantic Segmentation\n",
    "---\n",
    "In this part you are going to compare classic methods for segmentation to deep learning-based methods. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the images in the `./data/frogs` and `./data/horses` folders and display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(images_path):\n",
    "  '''\n",
    "  Loads the images from the given path to an array\n",
    "  -----------------------------------------------------------------\n",
    "  Inputs:\n",
    "  image_path     |     Path to the folder containing the images\n",
    "  -----------------------------------------------------------------\n",
    "  Output:\n",
    "  images         |     Array of images\n",
    "  '''\n",
    "  images = []\n",
    "  ims = os.listdir(images_path)\n",
    "  for im in ims:\n",
    "    image = images_path + im\n",
    "    if image is not None:\n",
    "      images.append(image)\n",
    "    \n",
    "  return images\n",
    "\n",
    "def show_images(images_path, labels=None):\n",
    "  '''\n",
    "  Show images with their labels (if they have any)\n",
    "  -----------------------------------------------------------------\n",
    "  Inputs:\n",
    "  image_path     |     Path to the folder containing the images\n",
    "  labels         |     Labels of the images \n",
    "  -----------------------------------------------------------------\n",
    "  Output:        \n",
    "  Plot the images with their labels     \n",
    "  '''\n",
    "  fig, axes = plt.subplots(1,len(images_path),figsize=(14,6)) # create a figure object\n",
    "  \n",
    "  if(type(images_path[0])==str and np.size(images_path) == 1):\n",
    "    if(type(images_path[0])==str):\n",
    "      image = Image.open(images_path[0])\n",
    "    else:\n",
    "      image = images_path[0]\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "\n",
    "    if (labels):\n",
    "      plt.title(labels)\n",
    "\n",
    "  else:\n",
    "    for idx, image_path in enumerate(images_path):\n",
    "      if(type(image_path)==str):\n",
    "        image = Image.open(image_path)\n",
    "      else:\n",
    "        image = image_path\n",
    "\n",
    "      axes[idx].imshow(image)\n",
    "\n",
    "      if (labels):\n",
    "        axes[idx].set_title(labels[idx])\n",
    "      \n",
    "      axes[idx].set_xticks([])\n",
    "      axes[idx].set_yticks([])\n",
    "      axes[idx].axis('off')\n",
    "\t  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to the folders\n",
    "frogs_path = '../data/frogs/'\n",
    "horses_path = '../data/horses/'\n",
    "\n",
    "# Load the images\n",
    "frogs_images = load_images(frogs_path)\n",
    "horses_images = load_images(horses_path)\n",
    "\n",
    "\n",
    "# Labels for images\n",
    "frogs_labels = ['Frog 1', 'Frog 2']\n",
    "horses_labels = ['Horse 1', 'Horse 2']\n",
    "\n",
    "# Show the images\n",
    "show_images(frogs_images, frogs_labels)\n",
    "show_images(horses_images, horses_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Pick 1 classic method for segmentation and 1 deep learning-based method and segment the given images. Display the results.\n",
    "    * **Briefly** summarize each method you picked and discuss the advantages and disadvantages of each method. In your answer, relate to the results you received in this section.\n",
    "    * You can use a ready implementation from the internet or OpenCV, no need to implement it yourselves.\n",
    "    * Note: the classic method **must not** use any neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classic method - GrabCut**\n",
    "\n",
    "GrabCut is an image segmentation method based on graph cuts. \n",
    "\n",
    "([Here](https://docs.opencv.org/3.4/d8/d83/tutorial_py_grabcut.html) is a good explanation of the algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabCut(image, rect):\n",
    "  '''\n",
    "  Show images with their labels (if they have any)\n",
    "  -----------------------------------------------------------------\n",
    "  Inputs:\n",
    "  image          |     Input image\n",
    "  rects          |     Coordinates of a rectangle that includes the foreground object\n",
    "                 |      x coordinates - Top left corner\n",
    "                 |      y coordinates - Top left corner\n",
    "                 |      w - Width of the rectangle\n",
    "                 |      h - Height of the rectangle\n",
    "  -----------------------------------------------------------------\n",
    "  Output: \n",
    "  masked         |     Segmented image       \n",
    "  Plot the images with their labels     \n",
    "  '''\n",
    "  mask = np.zeros(image.shape[:2],np.uint8) # Mask image where we specify which areas are background\n",
    "\n",
    "  # These arrays used by the algorithm internally.\n",
    "  bgdModel = np.zeros((1,65),np.float64)\n",
    "  fgdModel = np.zeros((1,65),np.float64)\n",
    "\n",
    "  cv2.grabCut(image,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "  mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8') \n",
    "  image = image*mask2[:,:,np.newaxis]\n",
    "  \n",
    "  return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick the RoI of the Images (Need to be done manualy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the image\n",
    "\n",
    "# img = frogs_images[0]\n",
    "# img = frogs_images[1]\n",
    "# img = horses_images[0]\n",
    "# img = horses_images[1]\n",
    "\n",
    "#image = cv2.imread(img)\n",
    "\n",
    "#roi= cv2.selectROI(\"select the area\", image)\n",
    "#print(roi)\n",
    "\n",
    "# Frog 1 RoI - (117, 91, 281, 253)\n",
    "# Frog 2 RoI - (163, 145, 185, 154)\n",
    "# Horse 1 RoI - (30, 3, 865, 573)\n",
    "# Horse 2 RoI - (611, 191, 401, 499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frog_1 = Image.open('../data/frogs/frog1.jpg')\n",
    "frog_2 = Image.open('../data/frogs/frog2.jpg')\n",
    "horse_1 = Image.open('../data/horses/horse1.png')\n",
    "horse_2 = Image.open('../data/horses/horse2.jpg')\n",
    "\n",
    "images = [frog_1, frog_2, horse_1, horse_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using classic method - GrabCut\n",
    "\n",
    "#x,y,w,h coordinates of the images - from the picked RoI (before)\n",
    "frogs_rects = [(117, 90, 281, 253), (163, 145, 185, 154)]\n",
    "horses_rects = [(30, 3, 865, 573), (611, 191, 401, 499)]\n",
    "\n",
    "labels = frogs_labels + horses_labels\n",
    "rects = frogs_rects + horses_rects\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "for i,img in enumerate(images):\n",
    "\n",
    "  npImage = np.array(img)            # Numpy array\n",
    "  im_crop = grabCut(npImage,rects[i]) # Use grabcut algorithm\n",
    "  \n",
    "  # Plot results\n",
    "  ax = fig.add_subplot(2,2,i+1)\n",
    "  ax.imshow(im_crop)\n",
    "  ax.set_title(labels[i])\n",
    "  ax.set_axis_off()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess(image):\n",
    "    # Converts pre-processed image to a plotable one\n",
    "    # -----------------------------------------------------------------\n",
    "    # Inputs:\n",
    "    # image     |     Processed image to deprocess\n",
    "    # -----------------------------------------------------------------\n",
    "    # output:\n",
    "    # A plotable version of the input image\n",
    "    \n",
    "    \n",
    "    # Reverse image adaption for model statistics \n",
    "    image = image[0].cpu().numpy() * np.array([[[0.229, 0.224, 0.225]]]).T + np.array([[[0.485, 0.456, 0.406]]]).T\n",
    "    return np.minimum(1,np.maximum(0,image.transpose(1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeplabv3_segmentation(images_path):\n",
    "  \"\"\"\n",
    "  this function preform semantic segmentation on all images in a given path\n",
    "  preform with black background and colorpallet\n",
    "  input\n",
    "    images_path: path for the dir with all images\n",
    "  output:\n",
    "  segmented: a list of segmented images with black background\n",
    "   segmented_colors: a list of segmented images with colors\n",
    "  \"\"\"\n",
    "  # Download and load the pre-trained model\n",
    "  model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet101', pretrained=True)   \n",
    "  model.eval(); # Inference mode\n",
    "\n",
    "  # Define the pre-processing steps\n",
    "  preprocess = transforms.Compose([\n",
    "      transforms.ToTensor(), # Image to tensor\n",
    "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # Normalization\n",
    "  ])\n",
    "  \n",
    "  \n",
    "  frames =  sorted(os.listdir(images_path)) # Put all the paths in array\n",
    "  segmented=[] # Will contain the output\n",
    "  segmented_colors = [] # Output colorpalete\n",
    "  for frame in frames:\n",
    "\n",
    "    input_image = Image.open(images_path + frame)\n",
    "    print(\"\\n \\n \\n \\n **********\")\n",
    "    print(input_image)\n",
    "    print(\"\\n \\n \\n \\n **********\")\n",
    "    input_image_array = imread(images_path + frame)\n",
    "\n",
    "    # Pre-processing\n",
    "    input_tensor = preprocess(input_image)\n",
    "    input_batch = input_tensor.unsqueeze(0) # Create a mini-batch of size 1 as expected by the model\n",
    "\n",
    "    # Send to device\n",
    "    model = model.to(device)\n",
    "    input_batch = input_batch.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)['out'][0]\n",
    "    output_predictions = output.argmax(0)\n",
    "\n",
    "    #create a mask to delete background\n",
    "    masked_seg = np.array(output_predictions.byte().cpu().numpy(), dtype=np.bool)\n",
    "    input_image_array[masked_seg == 0 ] = 0\n",
    "    \n",
    "    # create a color pallette, selecting a color for each class\n",
    "    palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n",
    "    colors = torch.as_tensor([i for i in range(21)])[:, None] * palette\n",
    "    colors = (colors % 255).numpy().astype(\"uint8\")\n",
    "    # plot the semantic segmentation predictions of 21 classes in each color\n",
    "    r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)\n",
    "    r.putpalette(colors)\n",
    "     \n",
    "    segmented_colors.append(r)\n",
    "    segmented.append(input_image_array)\n",
    "  return segmented, segmented_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, dl_seg_f = deeplabv3_segmentation(frogs_path)\n",
    "_, dl_seg_h = deeplabv3_segmentation(horses_path)\n",
    "\n",
    "show_images(dl_seg_f)\n",
    "show_images(dl_seg_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Pick 3 images (download from the internet or take them yourself) that satisfy the following, and dispaly them:\n",
    "    * One image of a living being (human, animal,...).\n",
    "    * One image of commonly-used object (car, chair, smartphone, glasses,...).\n",
    "    * One image of not-so-commonly-used object (fire extinguisher, satellite,... **BE CREATIVE**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my images\n",
    "animal_image = Image.open('../data/my_data/Humpback_Whale_underwater_shot.jpg')\n",
    "object_image = Image.open('../data/my_data/Credit_card_samp.jpg')\n",
    "uncommon_image = Image.open('../data/my_data/Nic_Cage_Pillow.jpg')\n",
    "\n",
    "my_images = [animal_image,object_image,uncommon_image] \n",
    "my_labels = ['Whale', 'Credit Cards','Nick Cage Pillow']\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "# plot\n",
    "for i,im in enumerate(my_images):\n",
    "  ax = fig.add_subplot(1,3,i+1)\n",
    "  ax.imshow(im)\n",
    "  ax.set_title(my_labels[i])\n",
    "  ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Apply each method (one classic and one deep learning-based) on the 3 images. Display the results (mask and segmented image).\n",
    "    * Which method performed better on each image? Describe your thoughts on why one method is better than the other.\n",
    "    * For the classic method you can change parameters per-image, document them in the report.\n",
    "    * You can add manual post-processing to get a mask if needed. If you do that, document in your report \"how hard\" you had to work in the post-processing stage, as it's an indication of the quality of the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. As you probably have noticed, segmentation can be rough around the edges, i.e., the mask is not perfect and may be noisy around the edges. What can be done to fix or at least alleviate this problem? Your suggestions can be in pre-processing, inside the segmentation algorithm or in post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
